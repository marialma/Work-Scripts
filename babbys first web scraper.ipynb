{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source had to be manually scraped because the page information was generated from javascript, and I could not get Selenium working for some reason. It wasn't too much to manually scrape anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fda = open('aggregated.txt','r')\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas\n",
    "fda_read = BeautifulSoup(fda, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application_numbers consists of a list of the numbers I'm interested in getting. \n",
    "# these numbers are in the aggregate.txt file. I want to use this list to find the urls associated with those numbers.\n",
    "\n",
    "appnums = open('application_numbers.txt','r')\n",
    "numlist = appnums.read()\n",
    "numlist = numlist.splitlines()\n",
    "appnums.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lists, populate lists if item matches. \n",
    "linklist = []\n",
    "appnumlist = []\n",
    "datelist = []\n",
    "tablelist = fda_read.find_all(id=\"foi\")\n",
    "for row in tablelist:\n",
    "    nada = row.find(\"a\", string=re.compile(r'\\b(?:%s)\\b' % '|'.join(numlist)))\n",
    "    if nada != None:\n",
    "        appnumlist.append(row.find(string=re.compile(r'\\b(?:%s)\\b' % '|'.join(numlist))))\n",
    "        linkid = nada.get('href')\n",
    "        linklist.append(linkid)\n",
    "        datelist.append(row.find(class_=\"col-sm-3 ng-binding\").find_next(class_=\"col-sm-3 ng-binding\").get_text(strip = True))\n",
    "# Clean up application numbers\n",
    "appnumlist = [a[-7:] for a in appnumlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV by mashing all the lists into a df\n",
    "df = pandas.DataFrame(appnumlist, columns =['Application Number'])\n",
    "df['Link']= linklist\n",
    "df['Date']= datelist\n",
    "df.to_csv('appdates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of application numbers that were not found. \n",
    "fulllist = set(numlist)\n",
    "foundlist = set(appnumlist)\n",
    "notfound = list(fulllist.difference(foundlist))\n",
    "dfnf = pandas.DataFrame(notfound, columns =['Items Not Found'])\n",
    "dfnf.to_csv('appdates_notfound.csv')\n",
    "# Can I generate the .csv without having to make a df first? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hooray~ i have written my first web scraper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
